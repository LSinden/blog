---
layout:     post
title:      "J.A.D.E."
date:       2016-08-25
summary:    Our new platform for analysing out BIIIIG DATAAAA...
category:   Technology
author:     Niall
project:    'hadoop'
thumbnail:  
header: 
---

We've been [working on cloud based suite of technology that we hope will let people work with huge data sets in a way that's user friendly but powerful][jade].

For a while now, we've been thinking about how we tackle the problem of "too-big-data". The Met Office data archive is many hundreds of Petabytes and it's growing ever faster as our computers get more powerful. Our current data analysis techniques can't keep up with these data volumes. We need to think about analysis in a fundamentally different way so we can continue to find the useful information in data. We aren't alone in this: "big-data" has become central to lots of industries, from health care to engineering and commerce.

Over the last couple of weeks, [Jacob](http://www.informaticslab.co.uk/profiles/jacob-tomlinson.html), I and the rest of the Lab dived into [Infrastructure as Code][IaC], tonnes of [Docker][docker], [Jupyter][jupyter], [Dask][dask] and more to see what we could come up with.

## What's wrong with the status quo?
In the past, chip manufacturers (think Intel, not McCaines) seemed to be able to make [ever faster processors][mooreslaw], meaning computers could do their sums faster, and we could generate and analyse more and more data.

### We're making a lot of data
At the Met Office, we have been using super-computers to generate data such as our weather forecasts since the 1950s. Super-computers are super because they comprise lots of different mini computers (or *nodes*). This means we can divide jobs up into lots of chunks, send each chunk to one node, and do them all at the same time - a technique known as *paralelliseation*. But currently we only use super-computers to *generate* data, and not to analyse it.

Traditionally processor speed has been fast enough to let us analyse this data, but chip manufacturers have announced that we can no longer expect chip speed to keep increasing. A few years ago, data analysis became compute bound - you couldn't just rely on ever faster chips to cope with the increasing data volumes.

### Paralell processing to the rescue?
 People have been dealing with this by paralellising their analysis calculations, which, for a while made-up for the stall in chip speed. However, the data volumes have now become so big that getting the data from storage to your nodes has become the new bottle-neck: the problem's now data-transfer bound.

### Total Parallelisation
So, if it takes ages to get your data to your calculation, what's the answer? Well, to take your calculation to your data, of course! If you store a bit of your data on each compute node, it don't need to wait for it to be sent there. Instead of just having paralellised compute, you now have paralellised data retrieval as well, which removes the final bottle-neck. A lot of very powerful systems, such as ([Hadoop][hadoop], [Spark][spark], [Dask][dask]) have sprung up over that last few years to try and do something like this.

## What have you done? 
Meet [J.A.D.E.][jade] - the Jupyter and Dask Environment, (or Jupyter Analysis of Data Environment, Just Another Data Environment, Jolly Advances Data Engine...I dunno, you decide). It let's users use Jupyter Notebooks (interactive data analysis in a web page) to write big distributed data analysis calculations. (WARNING: if you aren't interested in architecture diagrams, I don't blame you - don't be put off). I'm about to point you at the cool bits.

![JADE](https://s3-eu-west-1.amazonaws.com/informatics-webimages/Jade.png)

## The cool bits

#### Infrastructure as Code
As you can see from the cool diagram, JADE is made up of lots of separate servers all carefully wired together (we are using AWS for this prototype). For the first time, we've automated this process using a language called [Tarraform][terraform]. We can now start, stop, tweak, reproduce, share and move this system at the click of a button - an approach called [Infrastructure as Code][IaC].

#### Jupyterhub+Docker Swarm
This lets us automatically create new work environments for users as they log on to the system. Each user works in their own [Docker][docker] scientific environment (ours is [here]), meaning they can install any software they want, and generally tweak their system as much as they like without being a danger to other users.

 We got a lot of great advice from [this blog post](https://developer.rackspace.com/blog/deploying-jupyterhub-for-education/).

**Autoscaling**: by wrapping the Jupyter servers in an AWS Autoscaling Group, we can automatically scale the number of servers we have if there is a spike in the number of users.

**[Dask][dask]**: We've tried many of the different big-data engines out there. Currently, we're really excited by new-kid-on-the-block: [Dask][dask], which looks like it might be more suitable for us than the two really well established platforms, [Hadoop][hadoop] and [Spark][spark]. Our scientists are always dreaming of new involved ways to process our data. As such, we wanted a platform which is really flexible and adaptable. [Dask][dask] seems to have this advantage over [Hadoop][hadoop], which is really focused on relatively standard batch jobs. In addition, our scientists mostly work with the Python language. Again, this makes [Dask][dask] which is a suite of Python modules, the obvious choice as opposed to [Hadoop][hadoop] and [Spark][spark], which *can* work with Python, but are naturally geared towards Java, Scala and other languages.


[IaC]: https://en.wikipedia.org/wiki/Infrastructure_as_Code
[docker]: https://www.docker.com/
[jupyter]: http://jupyter.org/
[dask]: http://dask.pydata.org/en/latest/
[mooreslaw]: https://en.wikipedia.org/wiki/Moore%27s_law
[hadoop]: http://hadoop.apache.org/
[spark]: https://spark.apache.org/
[jade]: https://github.com/met-office-lab/jade
[terraform]: https://www.terraform.io/
[dockerswarm]: https://docs.docker.com/swarm/